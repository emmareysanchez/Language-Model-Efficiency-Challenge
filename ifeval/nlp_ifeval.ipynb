{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install absl-py langdetect nltk immutabledict datasets bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a933499673f641a481b141119206b499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from peft import prepare_model_for_kbit_training, PeftConfig, PeftModel\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"google/IFEval\")\n",
    "\n",
    "model_name = 'mistralai/Mistral-7B-v0.3'\n",
    "checkpoint_path = '../models/model10/output_lima/checkpoint-100'\n",
    "model_name = \"Qwen/Qwen2.5-7B\"\n",
    "checkpoint_path = \"../models/model11/output_oasst1/checkpoint-2000\"\n",
    "\n",
    "# Step 1: Load the tokenizer and model with quantization\n",
    "# model_name = \"models/model1\"  # Near 3B model (smallest available Qwen model)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=True,\n",
    "    padding_side='left'\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Enable loading the model in 4-bit precision\n",
    "    bnb_4bit_quant_type=\"nf4\",            # Specify quantization type as Normal Float 4\n",
    "    bnb_4bit_compute_dtype=getattr(torch, \"bfloat16\"), # Set computation data type\n",
    "    bnb_4bit_use_double_quant=True,       # Use double quantization for better accuracy\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(checkpoint_path)\n",
    "model = PeftModel.from_pretrained(model, checkpoint_path)\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Disable gradients to save memory and computation\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)  # Disable gradient computation globally\n",
    "\n",
    "# Prepare the output file\n",
    "output_dir = \"ifeval_responses\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "num_model = checkpoint_path.split('/')[-3].split(\"model\")[-1]\n",
    "output_file = os.path.join(output_dir, f\"input_response_data{num_model}.jsonl\")\n",
    "\n",
    "# Batch processing\n",
    "batch_size = 8  # Adjust based on your GPU memory capacity\n",
    "max_length = 128  # Limit output length to avoid excessive memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from datasets import load_dataset\n",
    "# from transformers import (\n",
    "#     AutoModelForCausalLM,\n",
    "#     AutoTokenizer,\n",
    "#     BitsAndBytesConfig\n",
    "# )\n",
    "# from tqdm import tqdm\n",
    "# import json\n",
    "\n",
    "# # Device\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # Load the dataset\n",
    "# dataset = load_dataset(\"google/IFEval\")\n",
    "\n",
    "# # Model name\n",
    "# model_name = \"../models/model10/output_lima/checkpoint-1000\"\n",
    "# # model_name = \"Qwen/Qwen2.5-7B\"\n",
    "\n",
    "# # Load the tokenizer for Mistral\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     model_name,\n",
    "#     add_eos_token=True,\n",
    "#     use_fast=True,\n",
    "#     padding_side='left',\n",
    "# )\n",
    "# tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS token\n",
    "\n",
    "# # Quantization configuration using bitsandbytes library\n",
    "# compute_dtype = getattr(torch, \"bfloat16\")\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=compute_dtype,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "# # Load the pre-trained model with the specified quantization configuration\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id  # Set the model's padding token ID\n",
    "\n",
    "# # Disable gradients to save memory and computation\n",
    "# model.eval()\n",
    "# torch.set_grad_enabled(False)  # Disable gradient computation globally\n",
    "\n",
    "# # Prepare the output file\n",
    "# output_dir = \"data\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# output_file = os.path.join(output_dir, \"input_response_data8.jsonl\")\n",
    "\n",
    "# # Batch processing\n",
    "# batch_size = 8  # Adjust based on your GPU memory capacity\n",
    "# max_length = 128  # Limit output length to avoid excessive memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_batch_prompt(batch_prompts):\n",
    "    # We add at the beginning of each prompt the token for the end of the previous prompt \"<Prompt>:\"\n",
    "    # and at the end \" \\n<Response>:\"\n",
    "    batch_prompts = [f\"<Prompt>: {prompt} \\n<Response>:\" for prompt in batch_prompts]\n",
    "    return tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|          | 0/68 [00:00<?, ?batch/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "2024-11-24 14:21:19.442333: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-24 14:21:19.451644: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-24 14:21:19.460080: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-24 14:21:19.462476: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-24 14:21:19.472392: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-24 14:21:19.951783: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Processing Batches:   1%|▏         | 1/68 [00:29<32:40, 29.26s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Bad pipe message: %s [b'\"Microsoft Edge\";v=\"131\", \"Chromium\";v=\"131\", \"Not_A Bran']\n",
      "Bad pipe message: %s [b';v=\"24\"\\r\\nsec-ch-ua-mobile: ?0\\r\\nsec-ch-ua-platform: \"Windows\"\\r\\nUpgrade-Insecure-Requests: 1\\r\\nUser-Ag']\n",
      "Bad pipe message: %s [b't: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0']\n",
      "Bad pipe message: %s [b'afari/537.36 Edg/131.0.0.0\\r\\nAcc', b't: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,']\n",
      "Bad pipe message: %s [b'plication/signed-exchange;v=b3;q=0.7\\r\\nSec-Fetch-Site: none\\r\\nSec-Fetch-Mode: navigate\\r\\nSec-Fetch-']\n",
      "Bad pipe message: %s [b'ol: max-age=0\\r\\nsec-ch-ua: \"Microsoft Edge\";v=\"131\", \"Chromium\";v=\"131\", \"Not_A Brand\";v=\"24\"\\r\\nsec-ch-ua-mobile: ?0\\r']\n",
      "Bad pipe message: %s [b'ec-ch-ua-', b'atform: \"Windows\"\\r\\nUpgrade-Insecure-Requests: 1\\r\\nUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWeb']\n",
      "Bad pipe message: %s [b't/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0\\r', b'ccept: te', b'/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchang']\n",
      "Processing Batches:   3%|▎         | 2/68 [00:57<31:18, 28.46s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:   4%|▍         | 3/68 [01:25<30:41, 28.34s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:   6%|▌         | 4/68 [01:52<29:54, 28.05s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:   7%|▋         | 5/68 [02:20<29:10, 27.78s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:   9%|▉         | 6/68 [02:47<28:33, 27.64s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  10%|█         | 7/68 [03:15<28:01, 27.57s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  12%|█▏        | 8/68 [03:42<27:28, 27.47s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  13%|█▎        | 9/68 [04:09<26:59, 27.45s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  15%|█▍        | 10/68 [04:37<26:32, 27.46s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  16%|█▌        | 11/68 [05:04<26:03, 27.43s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  18%|█▊        | 12/68 [05:32<25:39, 27.49s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  19%|█▉        | 13/68 [05:59<25:06, 27.39s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  21%|██        | 14/68 [06:27<24:55, 27.70s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  22%|██▏       | 15/68 [06:55<24:34, 27.82s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  24%|██▎       | 16/68 [07:23<24:09, 27.88s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  25%|██▌       | 17/68 [07:51<23:44, 27.92s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  26%|██▋       | 18/68 [08:20<23:20, 28.01s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  28%|██▊       | 19/68 [08:48<22:53, 28.03s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  29%|██▉       | 20/68 [09:16<22:24, 28.01s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  31%|███       | 21/68 [09:44<21:54, 27.97s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  32%|███▏      | 22/68 [10:12<21:27, 28.00s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  34%|███▍      | 23/68 [10:40<21:03, 28.07s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  35%|███▌      | 24/68 [11:09<20:53, 28.49s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  37%|███▋      | 25/68 [11:37<20:19, 28.36s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  38%|███▊      | 26/68 [12:05<19:43, 28.17s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  40%|███▉      | 27/68 [12:33<19:12, 28.12s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  41%|████      | 28/68 [13:01<18:44, 28.12s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  43%|████▎     | 29/68 [13:29<18:17, 28.13s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  44%|████▍     | 30/68 [13:57<17:48, 28.11s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  46%|████▌     | 31/68 [14:25<17:18, 28.05s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  47%|████▋     | 32/68 [14:53<16:50, 28.07s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  49%|████▊     | 33/68 [15:22<16:22, 28.07s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  50%|█████     | 34/68 [15:49<15:53, 28.03s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  51%|█████▏    | 35/68 [16:17<15:23, 27.99s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  53%|█████▎    | 36/68 [16:46<14:58, 28.09s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  54%|█████▍    | 37/68 [17:14<14:32, 28.15s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  56%|█████▌    | 38/68 [17:42<14:04, 28.16s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  57%|█████▋    | 39/68 [18:10<13:35, 28.11s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  59%|█████▉    | 40/68 [18:38<13:05, 28.04s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  60%|██████    | 41/68 [19:06<12:37, 28.07s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  62%|██████▏   | 42/68 [19:34<12:09, 28.07s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  63%|██████▎   | 43/68 [20:02<11:40, 28.04s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  65%|██████▍   | 44/68 [20:30<11:12, 28.03s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  66%|██████▌   | 45/68 [21:01<11:03, 28.83s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  68%|██████▊   | 46/68 [21:29<10:27, 28.54s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  69%|██████▉   | 47/68 [21:56<09:53, 28.28s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  71%|███████   | 48/68 [22:24<09:22, 28.14s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  72%|███████▏  | 49/68 [22:52<08:51, 27.98s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  74%|███████▎  | 50/68 [23:20<08:25, 28.11s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  75%|███████▌  | 51/68 [23:48<07:57, 28.09s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  76%|███████▋  | 52/68 [24:16<07:27, 27.99s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  78%|███████▊  | 53/68 [24:44<06:58, 27.89s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  79%|███████▉  | 54/68 [25:11<06:28, 27.78s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  81%|████████  | 55/68 [25:39<06:00, 27.76s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  82%|████████▏ | 56/68 [26:07<05:33, 27.81s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  84%|████████▍ | 57/68 [26:35<05:07, 27.92s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  85%|████████▌ | 58/68 [27:03<04:38, 27.88s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  87%|████████▋ | 59/68 [27:30<04:09, 27.69s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  88%|████████▊ | 60/68 [27:58<03:41, 27.64s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  90%|████████▉ | 61/68 [28:25<03:12, 27.55s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  91%|█████████ | 62/68 [28:52<02:44, 27.50s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  93%|█████████▎| 63/68 [29:20<02:17, 27.45s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  94%|█████████▍| 64/68 [29:47<01:49, 27.38s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  96%|█████████▌| 65/68 [30:14<01:22, 27.41s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  97%|█████████▋| 66/68 [30:42<00:54, 27.42s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches:  99%|█████████▊| 67/68 [31:09<00:27, 27.42s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Batches: 100%|██████████| 68/68 [31:36<00:00, 27.88s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses saved to data/input_response_data11.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(output_file, 'w') as f:\n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(dataset['train']), batch_size), desc=\"Processing Batches\", unit=\"batch\"):\n",
    "        try:\n",
    "            if (i + batch_size) > len(dataset['train']):\n",
    "                batch_prompts = dataset['train']['prompt'][i:len(dataset['train'])]\n",
    "            else:\n",
    "                batch_prompts = dataset['train']['prompt'][i:i + batch_size]\n",
    "            # Tokenize inputs\n",
    "            # inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            inputs = tokenize_batch_prompt(batch_prompts)\n",
    "            \n",
    "            # Generate responses\n",
    "            with torch.no_grad():  # Ensure gradients are disabled during generation\n",
    "                if max_length:\n",
    "                    outputs = model.generate(**inputs, max_new_tokens=max_length)\n",
    "                else:\n",
    "                    outputs = model.generate(**inputs)\n",
    "            # Decode responses and remove the prompt\n",
    "            responses = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "            for j in range(len(responses)):\n",
    "                responses[j] = responses[j][len(f\"<Prompt>: {batch_prompts[j]} \\n<Response>:\"):]\n",
    "            # Write each response directly to the file\n",
    "            for prompt, response in zip(batch_prompts, responses):\n",
    "                f.write(json.dumps({\"prompt\": prompt, \"response\": response}) + '\\n')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i}: {e}\")\n",
    "\n",
    "print(f\"Responses saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
