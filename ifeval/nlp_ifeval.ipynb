{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install absl-py langdetect nltk immutabledict datasets bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3932c1f2d2ff45f3a82160e71745c9a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from peft import prepare_model_for_kbit_training, PeftConfig, PeftModel\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"google/IFEval\")\n",
    "\n",
    "model_name = 'mistralai/Mistral-7B-v0.3'\n",
    "checkpoint_path = '../models/model10/output_lima/checkpoint-100'\n",
    "\n",
    "# Step 1: Load the tokenizer and model with quantization\n",
    "# model_name = \"models/model1\"  # Near 3B model (smallest available Qwen model)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=True,\n",
    "    padding_side='right'\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Enable loading the model in 4-bit precision\n",
    "    bnb_4bit_quant_type=\"nf4\",            # Specify quantization type as Normal Float 4\n",
    "    bnb_4bit_compute_dtype=getattr(torch, \"bfloat16\"), # Set computation data type\n",
    "    bnb_4bit_use_double_quant=True,       # Use double quantization for better accuracy\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(checkpoint_path)\n",
    "model = PeftModel.from_pretrained(model, checkpoint_path)\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Disable gradients to save memory and computation\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)  # Disable gradient computation globally\n",
    "\n",
    "# Prepare the output file\n",
    "output_dir = \"data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "num_model = checkpoint_path.split('/')[-3].split(\"model\")[-1]\n",
    "output_file = os.path.join(output_dir, f\"input_response_data{num_model}.jsonl\")\n",
    "\n",
    "# Batch processing\n",
    "batch_size = 8  # Adjust based on your GPU memory capacity\n",
    "max_length = 128  # Limit output length to avoid excessive memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc34228f41744174b102bb39314cc00b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from datasets import load_dataset\n",
    "# from transformers import (\n",
    "#     AutoModelForCausalLM,\n",
    "#     AutoTokenizer,\n",
    "#     BitsAndBytesConfig\n",
    "# )\n",
    "# from tqdm import tqdm\n",
    "# import json\n",
    "\n",
    "# # Device\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # Load the dataset\n",
    "# dataset = load_dataset(\"google/IFEval\")\n",
    "\n",
    "# # Model name\n",
    "# model_name = \"../models/model10/output_lima/checkpoint-1000\"\n",
    "# # model_name = \"Qwen/Qwen2.5-7B\"\n",
    "\n",
    "# # Load the tokenizer for Mistral\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     model_name,\n",
    "#     add_eos_token=True,\n",
    "#     use_fast=True,\n",
    "#     padding_side='left',\n",
    "# )\n",
    "# tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS token\n",
    "\n",
    "# # Quantization configuration using bitsandbytes library\n",
    "# compute_dtype = getattr(torch, \"bfloat16\")\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=compute_dtype,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "# # Load the pre-trained model with the specified quantization configuration\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id  # Set the model's padding token ID\n",
    "\n",
    "# # Disable gradients to save memory and computation\n",
    "# model.eval()\n",
    "# torch.set_grad_enabled(False)  # Disable gradient computation globally\n",
    "\n",
    "# # Prepare the output file\n",
    "# output_dir = \"data\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# output_file = os.path.join(output_dir, \"input_response_data8.jsonl\")\n",
    "\n",
    "# # Batch processing\n",
    "# batch_size = 8  # Adjust based on your GPU memory capacity\n",
    "# max_length = 128  # Limit output length to avoid excessive memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_batch_prompt(batch_prompts):\n",
    "    # We add at the beginning of each prompt the token for the end of the previous prompt \"<Prompt>:\"\n",
    "    # and at the end \" \\n<Response>:\"\n",
    "    batch_prompts = [f\"<Prompt>: {prompt} \\n<Response>:\" for prompt in batch_prompts]\n",
    "    return tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|          | 0/68 [00:00<?, ?batch/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "2024-11-23 16:55:34.365279: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-23 16:55:34.372736: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-23 16:55:34.380652: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-23 16:55:34.382865: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-23 16:55:34.389655: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-23 16:55:34.856876: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Processing Batches:   1%|▏         | 1/68 [00:31<35:19, 31.64s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:   3%|▎         | 2/68 [01:01<33:30, 30.46s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:   4%|▍         | 3/68 [01:30<32:32, 30.04s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:   6%|▌         | 4/68 [02:00<31:52, 29.89s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:   7%|▋         | 5/68 [02:29<31:10, 29.68s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:   9%|▉         | 6/68 [02:59<30:31, 29.55s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  10%|█         | 7/68 [03:28<30:03, 29.56s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  12%|█▏        | 8/68 [03:58<29:41, 29.70s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  13%|█▎        | 9/68 [04:28<29:17, 29.79s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  15%|█▍        | 10/68 [04:59<28:57, 29.97s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  16%|█▌        | 11/68 [05:29<28:28, 29.98s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  18%|█▊        | 12/68 [05:59<28:06, 30.11s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  19%|█▉        | 13/68 [06:29<27:31, 30.03s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  21%|██        | 14/68 [07:00<27:15, 30.29s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  22%|██▏       | 15/68 [07:30<26:38, 30.17s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  24%|██▎       | 16/68 [07:59<26:00, 30.01s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  25%|██▌       | 17/68 [08:29<25:26, 29.93s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  26%|██▋       | 18/68 [08:59<25:00, 30.01s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  28%|██▊       | 19/68 [09:29<24:28, 29.98s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  29%|██▉       | 20/68 [09:59<23:56, 29.93s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  31%|███       | 21/68 [10:28<23:22, 29.84s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  32%|███▏      | 22/68 [10:58<22:49, 29.77s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  34%|███▍      | 23/68 [11:28<22:20, 29.80s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  35%|███▌      | 24/68 [12:00<22:15, 30.35s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  37%|███▋      | 25/68 [12:29<21:32, 30.07s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  38%|███▊      | 26/68 [12:58<20:48, 29.74s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  40%|███▉      | 27/68 [13:27<20:12, 29.58s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  41%|████      | 28/68 [13:57<19:41, 29.54s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  43%|████▎     | 29/68 [14:26<19:11, 29.52s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  44%|████▍     | 30/68 [14:55<18:38, 29.45s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  46%|████▌     | 31/68 [15:24<18:05, 29.35s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  47%|████▋     | 32/68 [15:54<17:36, 29.35s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  49%|████▊     | 33/68 [16:23<17:06, 29.33s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  50%|█████     | 34/68 [16:52<16:37, 29.34s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  51%|█████▏    | 35/68 [17:22<16:12, 29.46s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  53%|█████▎    | 36/68 [17:52<15:50, 29.70s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  54%|█████▍    | 37/68 [18:23<15:24, 29.84s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  56%|█████▌    | 38/68 [18:53<14:58, 29.96s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  57%|█████▋    | 39/68 [19:23<14:27, 29.93s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  59%|█████▉    | 40/68 [19:52<13:56, 29.87s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  60%|██████    | 41/68 [20:23<13:28, 29.93s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  62%|██████▏   | 42/68 [20:53<12:58, 29.95s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  63%|██████▎   | 43/68 [21:22<12:27, 29.90s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  65%|██████▍   | 44/68 [21:52<11:57, 29.91s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  66%|██████▌   | 45/68 [22:27<11:59, 31.27s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  68%|██████▊   | 46/68 [22:57<11:20, 30.91s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  69%|██████▉   | 47/68 [23:27<10:42, 30.60s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  71%|███████   | 48/68 [23:57<10:10, 30.50s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  72%|███████▏  | 49/68 [24:27<09:34, 30.26s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  74%|███████▎  | 50/68 [24:57<09:06, 30.37s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  75%|███████▌  | 51/68 [25:27<08:35, 30.33s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  76%|███████▋  | 52/68 [25:57<08:03, 30.23s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  78%|███████▊  | 53/68 [26:27<07:31, 30.12s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  79%|███████▉  | 54/68 [26:57<07:00, 30.07s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  81%|████████  | 55/68 [27:27<06:30, 30.06s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  82%|████████▏ | 56/68 [27:57<06:00, 30.06s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  84%|████████▍ | 57/68 [28:28<05:31, 30.13s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  85%|████████▌ | 58/68 [28:58<05:00, 30.06s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  87%|████████▋ | 59/68 [29:27<04:30, 30.00s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  88%|████████▊ | 60/68 [29:57<03:59, 29.99s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  90%|████████▉ | 61/68 [30:27<03:30, 30.01s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  91%|█████████ | 62/68 [30:57<02:59, 29.94s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  93%|█████████▎| 63/68 [31:27<02:29, 29.84s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  94%|█████████▍| 64/68 [31:56<01:59, 29.76s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  96%|█████████▌| 65/68 [32:26<01:29, 29.74s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  97%|█████████▋| 66/68 [32:56<00:59, 29.74s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  99%|█████████▊| 67/68 [33:26<00:29, 29.77s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches: 100%|██████████| 68/68 [33:54<00:00, 29.92s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses saved to data/input_response_data7.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(output_file, 'w') as f:\n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(dataset['train']), batch_size), desc=\"Processing Batches\", unit=\"batch\"):\n",
    "        try:\n",
    "            if (i + batch_size) > len(dataset['train']):\n",
    "                batch_prompts = dataset['train']['prompt'][i:len(dataset['train'])]\n",
    "            else:\n",
    "                batch_prompts = dataset['train']['prompt'][i:i + batch_size]\n",
    "            # Tokenize inputs\n",
    "            # inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            inputs = tokenize_batch_prompt(batch_prompts)\n",
    "            \n",
    "            # Generate responses\n",
    "            with torch.no_grad():  # Ensure gradients are disabled during generation\n",
    "                if max_length:\n",
    "                    outputs = model.generate(**inputs, max_new_tokens=max_length)\n",
    "                else:\n",
    "                    outputs = model.generate(**inputs)\n",
    "            # Decode responses and remove the prompt\n",
    "            responses = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "            for j in range(len(responses)):\n",
    "                responses[j] = responses[j][len(f\"<Prompt>: {batch_prompts[j]} \\n<Response>:\"):]\n",
    "            # Write each response directly to the file\n",
    "            for prompt, response in zip(batch_prompts, responses):\n",
    "                f.write(json.dumps({\"prompt\": prompt, \"response\": response}) + '\\n')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i}: {e}\")\n",
    "\n",
    "print(f\"Responses saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
